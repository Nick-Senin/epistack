"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ —Ç—Ä–æ–µ–∫ —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å –ø–æ–º–æ—â—å—é GEPA.
"""
from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any, Dict, List, Mapping, Optional, Tuple

import dotenv
import dspy

from data_models.state_triple import StateTriple
from epistack_data import for_abstraction_module
from .metrics import StateTripleSimilarityMetric
from .module import NaiveStateTripleAbstraction


DEFAULT_DATASET_PATH = Path(__file__).resolve().parents[1] / "datasets" / "abstraction_dataset.json"
OPTIMIZATION_MODEL_ID = "openrouter/moonshotai/kimi-k2-thinking"


def _clean_text(value: Any) -> Optional[str]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É: str -> strip -> None –µ—Å–ª–∏ –ø—É—Å—Ç–æ.
    """
    if value is None:
        return None
    cleaned = str(value).strip()
    return cleaned if cleaned else None


def _make_state_triple(
    initial: Optional[str],
    transformation: Optional[str],
    final_state: Optional[str],
) -> Optional[StateTriple]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç StateTriple –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç.
    """
    if not (initial and transformation and final_state):
        return None
    triple: StateTriple = {
        "initial_state": initial,
        "transformation": transformation,
        "final_state": final_state,
    }
    return triple


def _example_from_triples(source: StateTriple, target: StateTriple) -> dspy.Example:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä—É —Ç—Ä–æ–µ–∫ –≤ DSPy –ø—Ä–∏–º–µ—Ä.
    """
    return (
        dspy.Example(
            state_triple=source,
            abstract_state_triple=target,
        ).with_inputs("state_triple")
    )


def _local_record_examples(record: Dict[str, Any]) -> Tuple[List[dspy.Example], int]:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—Ä–∏–º–µ—Ä—ã, –ø—Ä–æ–ø—É—â–µ–Ω–æ).
    """
    examples: List[dspy.Example] = []
    skipped = 0
    index = 1

    while True:
        keys = (
            f"–°–í–Ø–ó–ö–ê {index} - initial_state",
            f"–°–í–Ø–ó–ö–ê {index} - transformation",
            f"–°–í–Ø–ó–ö–ê {index} - result",
            f"–°–í–Ø–ó–ö–ê {index} - –ê–ë–°–¢–†–ê–ö–¶–ò–Ø - initial_state",
            f"–°–í–Ø–ó–ö–ê {index} - –ê–ë–°–¢–†–ê–ö–¶–ò–Ø - transformation",
            f"–°–í–Ø–ó–ö–ê {index} - –ê–ë–°–¢–†–ê–ö–¶–ò–Ø - result",
        )
        has_data = any(_clean_text(record.get(key)) for key in keys)
        if not has_data:
            break

        source = _make_state_triple(
            _clean_text(record.get(keys[0])),
            _clean_text(record.get(keys[1])),
            _clean_text(record.get(keys[2])),
        )
        target = _make_state_triple(
            _clean_text(record.get(keys[3])),
            _clean_text(record.get(keys[4])),
            _clean_text(record.get(keys[5])),
        )

        if source and target:
            examples.append(_example_from_triples(source, target))
        else:
            skipped += 1

        index += 1

    return examples, skipped


def _load_local_dataset(dataset_path: Path) -> List[dspy.Example]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π abstraction_dataset.json –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≤ –ø—Ä–∏–º–µ—Ä—ã DSPy.
    """
    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")

    try:
        raw_records = json.loads(dataset_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –≤ {dataset_path}: {exc}") from exc

    examples: List[dspy.Example] = []
    skipped_total = 0

    for record in raw_records:
        record_examples, skipped = _local_record_examples(record or {})
        examples.extend(record_examples)
        skipped_total += skipped

    if not examples:
        raise ValueError(
            f"–í {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –ø–∞—Ä (–∏—Å—Ö–æ–¥–Ω–∞—è —Ç—Ä–æ–π–∫–∞ + –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è)."
        )

    print(
        f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} —Ç—Ä–æ–µ–∫ –∏–∑ {dataset_path} "
        f"(–ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped_total} –Ω–µ–ø–æ–ª–Ω—ã—Ö —Å–≤—è–∑–æ–∫)"
    )
    return examples


def _state_triple_from_mapping(payload: Mapping[str, Any]) -> Optional[StateTriple]:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤ StateTriple, —É—á–∏—Ç—ã–≤–∞—è —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–ª—é—á–µ–π.
    """
    initial = _clean_text(payload.get("initial_state") or payload.get("initial"))
    transformation = _clean_text(payload.get("transformation") or payload.get("action"))
    final_state = _clean_text(
        payload.get("final_state") or payload.get("result") or payload.get("final")
    )
    return _make_state_triple(initial, transformation, final_state)


def _hf_examples(hf_username: str) -> List[dspy.Example]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∏–∑ HF-–¥–∞—Ç–∞—Å–µ—Ç–∞ (extracted_chains/abstracted_chains).
    """
    dataset = for_abstraction_module(hf_username)
    examples: List[dspy.Example] = []
    skipped = 0

    for item in dataset:
        extracted = getattr(item, "extracted_chains", None) or []
        abstracted = getattr(item, "abstracted_chains", None) or []
        limit = min(len(extracted), len(abstracted))

        for idx in range(limit):
            source = _state_triple_from_mapping(extracted[idx])
            target = _state_triple_from_mapping(abstracted[idx])
            if source and target:
                examples.append(_example_from_triples(source, target))
            else:
                skipped += 1

    if not examples:
        raise ValueError(
            "HF-–¥–∞—Ç–∞—Å–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç—Ä–æ–µ–∫ (extracted_chains vs abstracted_chains)."
        )

    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(examples)} —Ç—Ä–æ–µ–∫ –∏–∑ HF –¥–∞—Ç–∞—Å–µ—Ç–∞ ({skipped} –ø—Ä–æ–ø—É—â–µ–Ω–æ)")
    return examples


def _load_examples(
    hf_username: Optional[str] = None,
    dataset_path: Optional[str] = None,
) -> List[dspy.Example]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: HF –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π JSON.
    """
    if hf_username:
        return _hf_examples(hf_username)
    target_path = Path(dataset_path) if dataset_path else DEFAULT_DATASET_PATH
    return _load_local_dataset(target_path)


def _split_dataset(examples: List[dspy.Example]) -> Tuple[List[dspy.Example], List[dspy.Example]]:
    """
    –î–µ–ª–∏—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/val (80/20) —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –º–∏–Ω–∏–º—É–º 1 –ø—Ä–∏–º–µ—Ä –≤ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏.
    """
    if len(examples) < 2:
        raise ValueError("–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 2 –ø—Ä–∏–º–µ—Ä–∞.")
    split_idx = max(1, int(len(examples) * 0.8))
    trainset = examples[:split_idx]
    valset = examples[split_idx:]
    if not valset:
        valset = trainset[-1:]
        trainset = trainset[:-1]
    print(f"üìä –î–∞—Ç–∞—Å–µ—Ç —Ç—Ä–æ–µ–∫: {len(trainset)} train, {len(valset)} val")
    return trainset, valset


def _configure_optimization_lm() -> dspy.LM:
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LLM –ø–æ–¥ GEPA-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é.
    """
    dotenv.load_dotenv()
    api_base = (
        os.getenv("OPENROUTER_API_BASE")
        or os.getenv("OPENROUTER_BASE")
        or "https://openrouter.ai/api/v1"
    )
    api_key = os.getenv("OPENROUTER_API_KEY", "")

    lm = dspy.LM(
        model=OPTIMIZATION_MODEL_ID,
        api_base=api_base,
        api_key=api_key,
    )
    dspy.configure(lm=lm)
    return lm


def optimize(
    hf_username: Optional[str] = None,
    max_metric_calls: int = 75,
    dataset_path: Optional[str] = None,
    reflection_minibatch_size: int = 3,
) -> NaiveStateTripleAbstraction:
    """
    GEPA-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è NaiveStateTripleAbstraction.
    
    Args:
        hf_username: HuggingFace username (–µ—Å–ª–∏ None ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π JSON).
        max_metric_calls: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∑–æ–≤–æ–≤ –º–µ—Ç—Ä–∏–∫–∏.
        dataset_path: –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É abstraction_dataset.json.
        reflection_minibatch_size: –†–∞–∑–º–µ—Ä –º–∏–Ω–∏–±–∞—Ç—á–∞ –¥–ª—è –æ—Ç—Ä–∞–∂–µ–Ω–∏—è GEPA.
        
    Returns:
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å NaiveStateTripleAbstraction.
    """
    examples = _load_examples(hf_username=hf_username, dataset_path=dataset_path)
    trainset, valset = _split_dataset(examples)

    optimization_lm = _configure_optimization_lm()
    metric = StateTripleSimilarityMetric()

    optimizer = dspy.GEPA(
        metric=metric,
        max_metric_calls=max_metric_calls,
        reflection_lm=optimization_lm,
        reflection_minibatch_size=reflection_minibatch_size,
        candidate_selection_strategy="pareto",
        skip_perfect_score=True,
        track_stats=True,
        seed=42,
    )

    module = NaiveStateTripleAbstraction()
    optimized = optimizer.compile(
        module,
        trainset=trainset,
        valset=valset,
    )

    print("‚úÖ NaiveStateTripleAbstraction –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω —Å GEPA")
    return optimized

