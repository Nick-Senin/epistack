"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–≤—è–∑–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GEPA
"""
import os
import json
import dotenv
import dspy
from pathlib import Path
from typing import Optional, List, Dict, Any

from epistack_data import for_extraction_module
from config import configure_llm
from .module import StateTransformationExtractor
from .metrics import create_metric, metric

# -------------------------------------------------------------------------------------------
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# -------------------------------------------------------------------------------------------

DEFAULT_DATASET_PATH = Path(__file__).resolve().parents[1] / "datasets" / "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ç–∏–≤—ã - —Ç—Ä–µ–Ω–∞–∂–µ—Ä–∫–∞ - Sheet1.json"
OPTIMIZATION_MODEL_ID = "openrouter/moonshotai/kimi-k2-thinking"


def _configure_optimization_lm():
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LLM –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –¥–ª—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
    """
    dotenv.load_dotenv()
    api_base = (
        os.getenv("OPENROUTER_API_BASE")
        or os.getenv("OPENROUTER_BASE")
        or "https://openrouter.ai/api/v1"
    )
    api_key = os.getenv("OPENROUTER_API_KEY", "")

    lm = dspy.LM(
        model=OPTIMIZATION_MODEL_ID,
        api_base=api_base,
        api_key=api_key,
    )
    # dspy.configure(lm=lm)  # –ù–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ, —Ç–æ–ª—å–∫–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏
    return lm


# -------------------------------------------------------------------------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ª–æ–∫–∞–ª—å–Ω–æ –∏–ª–∏ —á–µ—Ä–µ–∑ HF)
# -------------------------------------------------------------------------------------------

def _clean_text(value: Optional[str]) -> Optional[str]:
    if isinstance(value, str):
        cleaned = value.strip()
        return cleaned if cleaned else None
    return None


def _parse_chains(record: Dict[str, Any]) -> List[Dict[str, str]]:
    """
    –ü–∞—Ä—Å–∏—Ç –ø–ª–æ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–°–í–Ø–ó–ö–ê X - ...) –∏–ª–∏ —Å–ø–∏—Å–æ–∫ triples –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π.
    """
    chains = []
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –°–ø–∏—Å–æ–∫ triples (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç)
    if "triples" in record and isinstance(record["triples"], list):
        for item in record["triples"]:
            initial = _clean_text(item.get("–Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ") or item.get("initial_state"))
            transformation = _clean_text(item.get("–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ") or item.get("transformation"))
            final = _clean_text(item.get("—Ä–µ–∑—É–ª—å—Ç–∞—Ç") or item.get("final_state") or item.get("result"))
            
            if initial and transformation and final:
                chains.append({
                    "initial_state": initial,
                    "transformation": transformation,
                    "final_state": final
                })
        if chains:
            return chains

    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–ª–æ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 10 —Å–≤—è–∑–æ–∫ (–æ–±—ã—á–Ω–æ –∏—Ö 1-4)
    for i in range(1, 11):
        prefix = f"–°–í–Ø–ó–ö–ê {i}"
        
        # –ö–ª—é—á–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–∞–∑–Ω—ã–º–∏, –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        initial = _clean_text(record.get(f"{prefix} - initial_state"))
        transformation = _clean_text(record.get(f"{prefix} - transformation"))
        # –í –¥–∞—Ç–∞—Å–µ—Ç–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'result', –Ω–æ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç 'final_state'
        result = _clean_text(record.get(f"{prefix} - result") or record.get(f"{prefix} - final_state"))
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã —á–∞—Å—Ç–∏—á–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ, –ø—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å
        if initial or transformation or result:
            if initial and transformation and result:
                chains.append({
                    "initial_state": initial,
                    "transformation": transformation,
                    "final_state": result
                })
    return chains


def _get_mock_dataset() -> List[dspy.Example]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π (mock) –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
    """
    print("‚ö†Ô∏è –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø MOCK –î–ê–¢–ê–°–ï–¢ (—Ä–µ–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω) ‚ö†Ô∏è")
    
    mock_data = [
        {
            "source_text": "–ö–æ–º–ø–∞–Ω–∏—è —Ä–µ—à–∏–ª–∞ –≤–Ω–µ–¥—Ä–∏—Ç—å –Ω–æ–≤—É—é CRM-—Å–∏—Å—Ç–µ–º—É. –°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –ø—Ä–æ—à–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–∞—á–∞–ª–∏ –≤–Ω–æ—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–æ–¥–∞–∂–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 20% –∑–∞ –∫–≤–∞—Ä—Ç–∞–ª.",
            "extracted_chains": [
                {
                    "initial_state": "–í –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞–º–∏ (CRM)",
                    "transformation": "–ö–æ–º–ø–∞–Ω–∏—è –≤–Ω–µ–¥—Ä—è–µ—Ç –Ω–æ–≤—É—é CRM-—Å–∏—Å—Ç–µ–º—É –∏ –æ–±—É—á–∞–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ —Ä–∞–±–æ—Ç–µ —Å –Ω–µ–π",
                    "final_state": "–ü—Ä–æ–¥–∞–∂–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 20% –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"
                }
            ]
        },
        {
            "source_text": "–°—Ç—É–¥–µ–Ω—Ç –≥–æ—Ç–æ–≤–∏–ª—Å—è –∫ —ç–∫–∑–∞–º–µ–Ω—É –≤—Å—é –Ω–æ—á—å, —á–∏—Ç–∞—è –∫–æ–Ω—Å–ø–µ–∫—Ç—ã. –£—Ç—Ä–æ–º –æ–Ω –≤—ã–ø–∏–ª –∫—Ä–µ–ø–∫–∏–π –∫–æ—Ñ–µ. –ù–∞ —ç–∫–∑–∞–º–µ–Ω–µ –æ–Ω —á—É–≤—Å—Ç–≤–æ–≤–∞–ª —Å–µ–±—è –±–æ–¥—Ä—ã–º, –Ω–æ –∑–∞–±—ã–ª —á–∞—Å—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –∏–∑-–∑–∞ —É—Å—Ç–∞–ª–æ—Å—Ç–∏.",
            "extracted_chains": [
                {
                    "initial_state": "–°—Ç—É–¥–µ–Ω—Ç –∏–º–µ–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞–Ω–∏—è—Ö –ø–µ—Ä–µ–¥ —ç–∫–∑–∞–º–µ–Ω–æ–º",
                    "transformation": "–°—Ç—É–¥–µ–Ω—Ç –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ –≥–æ—Ç–æ–≤–∏—Ç—Å—è –≤—Å—é –Ω–æ—á—å",
                    "final_state": "–°—Ç—É–¥–µ–Ω—Ç —É—Å—Ç–∞–ª –∏ –∑–∞–±—ã–ª —á–∞—Å—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª–∞"
                },
                {
                    "initial_state": "–°—Ç—É–¥–µ–Ω—Ç —á—É–≤—Å—Ç–≤—É–µ—Ç —Å–æ–Ω–ª–∏–≤–æ—Å—Ç—å —É—Ç—Ä–æ–º",
                    "transformation": "–°—Ç—É–¥–µ–Ω—Ç –≤—ã–ø–∏–≤–∞–µ—Ç –∫—Ä–µ–ø–∫–∏–π –∫–æ—Ñ–µ",
                    "final_state": "–°—Ç—É–¥–µ–Ω—Ç —á—É–≤—Å—Ç–≤—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –±–æ–¥—Ä–æ—Å—Ç—å"
                }
            ]
        },
        {
            "source_text": "–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –æ–±–Ω–∞—Ä—É–∂–∏–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –±–∞–≥ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ. –û–Ω –æ—Ç–∫–∞—Ç–∏–ª –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–ª–∏–∑ –∏ –Ω–∞—á–∞–ª –∏—Å–∫–∞—Ç—å –æ—à–∏–±–∫—É –≤ –ª–æ–≥–∞—Ö. –ß–µ—Ä–µ–∑ —á–∞—Å —Ä–∞–±–æ—Ç–∞ —Å–µ—Ä–≤–∏—Å–∞ –±—ã–ª–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.",
            "extracted_chains": [
                {
                    "initial_state": "–í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –±–∞–≥, —Å–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ",
                    "transformation": "–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Ç–∫–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–∏–∑ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏",
                    "final_state": "–†–∞–±–æ—Ç–∞ —Å–µ—Ä–≤–∏—Å–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, –ø—Ä–∏—á–∏–Ω–∞ —Å–±–æ—è –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–∞"
                }
            ]
        }
    ]
    
    return [
        dspy.Example(
            source_text=item["source_text"],
            extracted_chains=item["extracted_chains"]
        ).with_inputs("source_text")
        for item in mock_data
    ]


def _load_local_dataset(dataset_path: Path) -> List[dspy.Example]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è module_extraction.
    –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç MOCK –¥–∞—Ç–∞—Å–µ—Ç.
    """
    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        print(f"‚ÑπÔ∏è –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {dataset_path}")
        return _get_mock_dataset()

    try:
        raw_records = json.loads(dataset_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON –≤ {dataset_path}: {exc}")
        return _get_mock_dataset()

    examples = []
    skipped = 0

    for record in raw_records:
        source_text = _clean_text(
            record.get("–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–∏–º–µ—Ä") 
            or record.get("source_text") 
            or record.get("case") 
            or record.get("–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–∏–º–µ—Ä–∞")
        )
        
        # –ü–∞—Ä—Å–∏–º —Ü–µ–ø–æ—á–∫–∏
        extracted_chains = _parse_chains(record)

        if not source_text or not extracted_chains:
            skipped += 1
            continue

        examples.append(
            dspy.Example(
                source_text=source_text,
                extracted_chains=extracted_chains,
            ).with_inputs("source_text")
        )

    if not examples:
        print(f"‚ö†Ô∏è –í {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.")
        return _get_mock_dataset()

    print(
        f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {dataset_path} "
        f"(–ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped} –∑–∞–ø–∏—Å–µ–π)"
    )
    return examples


# -------------------------------------------------------------------------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
# -------------------------------------------------------------------------------------------

def optimize(
    hf_username: Optional[str] = None,
    max_metric_calls: int = 50,
    dataset_path: Optional[str] = None,
):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–≤—è–∑–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GEPA.
    
    Args:
        hf_username: HuggingFace username –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (None = –ª–æ–∫–∞–ª—å–Ω—ã–π)
        max_metric_calls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∑–æ–≤–æ–≤ –º–µ—Ç—Ä–∏–∫–∏
        dataset_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞ (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
        
    Returns:
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å StateTransformationExtractor
    """
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if hf_username:
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ HuggingFace ({hf_username})...")
        dataset = for_extraction_module(hf_username)
    else:
        target_path = Path(dataset_path) if dataset_path else DEFAULT_DATASET_PATH
        print(f"üìÇ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {target_path}...")
        dataset = _load_local_dataset(target_path)
    
    if len(dataset) < 2:
        print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ MOCK –¥–∞—Ç–∞—Å–µ—Ç")
        dataset = _get_mock_dataset()
        
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val (80/20)
    # –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ mock –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å—å –¥–ª—è train –∏ val, –∏–ª–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å
    if len(dataset) <= 3:
         trainset = dataset
         valset = dataset
    else:
        split_idx = max(1, int(len(dataset) * 0.8))
        trainset = dataset[:split_idx]
        valset = dataset[split_idx:]
    
    print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {len(trainset)} train, {len(valset)} val")
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –∏ –ú–µ—Ç—Ä–∏–∫–∏
    # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ config (–≥–ª–æ–±–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞)
    configure_llm()
    
    # –ú–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è, Moonshot)
    reflection_lm = _configure_optimization_lm()
    
    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫—É —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é metric, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å GEPA –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç score/feedback
    gepa_metric = metric
    
    # 3. –ó–∞–ø—É—Å–∫ GEPA
    print("üöÄ –ó–∞–ø—É—Å–∫ GEPA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
    
    optimizer = dspy.GEPA(
        metric=gepa_metric,
        max_metric_calls=max_metric_calls,
        reflection_lm=reflection_lm,
        reflection_minibatch_size=3,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑
        candidate_selection_strategy='pareto',
        skip_perfect_score=True,
        track_stats=True,
        seed=42
    )
    
    module = StateTransformationExtractor()
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
    optimized = optimizer.compile(
        module,
        trainset=trainset,
        valset=valset
    )
    
    print("‚úÖ StateTransformationExtractor —É—Å–ø–µ—à–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return optimized
