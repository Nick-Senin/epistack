"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
"""
import os
import json
from pathlib import Path
from typing import Optional

import dotenv
import dspy
from epistack_data import for_naming_module
from .module import RelationNamer
from .metrics import create_metric


DEFAULT_DATASET_PATH = Path(__file__).resolve().parents[1] / "datasets" / "kollektives_dataset.json"


def _clean_text(value: Optional[str]) -> Optional[str]:
    if isinstance(value, str):
        cleaned = value.strip()
        return cleaned if cleaned else None
    return None


def _load_kollektives_dataset(dataset_path: Path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç kollektives_dataset.json –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–≥–æ –≤ dspy.Example.
    """
    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")

    try:
        raw_records = json.loads(dataset_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as exc:
        raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –≤ {dataset_path}: {exc}") from exc

    examples = []
    skipped = 0

    for record in raw_records:
        title = _clean_text(record.get("–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞") or record.get("title"))
        source_text = _clean_text(
            record.get("–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–∏–º–µ—Ä") or record.get("source_text") or record.get("case")
        )

        if not title or not source_text:
            skipped += 1
            continue

        examples.append(
            dspy.Example(
                source_text=source_text,
                title=title,
            ).with_inputs("source_text")
        )

    if not examples:
        raise ValueError(
            f"–í {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –∫–µ–π—Å–∞–º–∏ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏."
        )

    print(
        f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {dataset_path} "
        f"(–ø—Ä–æ–ø—É—â–µ–Ω–æ {skipped} –Ω–µ–ø–æ–ª–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π)"
    )
    return examples


OPTIMIZATION_MODEL_ID = "openrouter/moonshotai/kimi-k2-thinking"


def _configure_optimization_lm():
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LLM –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –¥–ª—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
    """
    dotenv.load_dotenv()
    api_base = (
        os.getenv("OPENROUTER_API_BASE")
        or os.getenv("OPENROUTER_BASE")
        or "https://openrouter.ai/api/v1"
    )
    api_key = os.getenv("OPENROUTER_API_KEY", "")

    lm = dspy.LM(
        model=OPTIMIZATION_MODEL_ID,
        api_base=api_base,
        api_key=api_key,
    )
    dspy.configure(lm=lm)
    return lm


def optimize(
    hf_username: Optional[str] = None,
    max_metric_calls: int = 50,
    dataset_path: Optional[str] = None,
):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GEPA
    
    Args:
        hf_username: HuggingFace username –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (None = –ª–æ–∫–∞–ª—å–Ω—ã–π)
        max_metric_calls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∑–æ–≤–æ–≤ –º–µ—Ç—Ä–∏–∫–∏
        dataset_path: –ü—É—Ç—å –∫ kollektives_dataset.json (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ)
        
    Returns:
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å RelationNamer
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ª–æ–∫–∞–ª—å–Ω—ã–π JSON –∏–ª–∏ HF)
    if hf_username:
        dataset = for_naming_module(hf_username)
    else:
        target_path = Path(dataset_path) if dataset_path else DEFAULT_DATASET_PATH
        dataset = _load_kollektives_dataset(target_path)
    
    if len(dataset) < 2:
        raise ValueError("–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 2 –ø—Ä–∏–º–µ—Ä–∞.")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val (80/20)
    split_idx = max(1, int(len(dataset) * 0.8))
    trainset = dataset[:split_idx]
    valset = dataset[split_idx:]
    
    print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {len(trainset)} train, {len(valset)} val")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –º–µ—Ç—Ä–∏–∫—É LLM as Judge
    optimization_lm = _configure_optimization_lm()
    metric = create_metric()
    
    # –°–æ–∑–¥–∞–µ–º LM –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é LM)
    reflection_lm = optimization_lm
    
    # GEPA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å —Ä–µ—Ñ–ª–µ–∫—Ç–∏–≤–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤
    optimizer = dspy.GEPA(
        metric=metric,
        max_metric_calls=max_metric_calls,
        reflection_lm=reflection_lm,
        reflection_minibatch_size=3,
        candidate_selection_strategy='pareto',
        skip_perfect_score=True,
        track_stats=True,
        seed=42
    )
    
    module = RelationNamer()
    optimized = optimizer.compile(
        module,
        trainset=trainset,
        valset=valset
    )
    
    print("‚úÖ RelationNamer –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω —Å GEPA")
    return optimized

