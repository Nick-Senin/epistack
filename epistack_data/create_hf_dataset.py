"""
–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ Hugging Face –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥—É–ª–µ–π epistack
"""
from datasets import Dataset, DatasetDict, Features, Value, Sequence
import pandas as pd
import os


def create_dataset_structure():
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
    
    examples = [
        {
            'source_text': '''def process_user_data(raw_data):
    cleaned = remove_nulls(raw_data)
    validated = check_format(cleaned)
    return save_to_db(validated)''',
            
            'title': 'User Data Processing Pipeline',
            
            'extracted_chains': [
                {
                    'initial_state': 'raw_data',
                    'transformation': 'remove_nulls',
                    'result': 'cleaned'
                },
                {
                    'initial_state': 'cleaned',
                    'transformation': 'check_format',
                    'result': 'validated'
                },
                {
                    'initial_state': 'validated',
                    'transformation': 'save_to_db',
                    'result': 'stored_data'
                }
            ],
            
            'abstracted_chains': [
                {
                    'initial_state': 'unprocessed_input',
                    'transformation': 'sanitize',
                    'result': 'clean_input'
                },
                {
                    'initial_state': 'clean_input',
                    'transformation': 'validate',
                    'result': 'valid_input'
                },
                {
                    'initial_state': 'valid_input',
                    'transformation': 'persist',
                    'result': 'stored_output'
                }
            ]
        },
        
        {
            'source_text': '''class EmailSender:
    def send(self, recipients, message):
        formatted = self.format_html(message)
        attachments = self.prepare_files()
        return self.smtp_send(recipients, formatted, attachments)''',
            
            'title': 'Email Delivery System',
            
            'extracted_chains': [
                {
                    'initial_state': 'message',
                    'transformation': 'format_html',
                    'result': 'formatted'
                },
                {
                    'initial_state': 'files',
                    'transformation': 'prepare_files',
                    'result': 'attachments'
                },
                {
                    'initial_state': 'recipients, formatted, attachments',
                    'transformation': 'smtp_send',
                    'result': 'send_result'
                }
            ],
            
            'abstracted_chains': [
                {
                    'initial_state': 'raw_content',
                    'transformation': 'format',
                    'result': 'formatted_content'
                },
                {
                    'initial_state': 'raw_resources',
                    'transformation': 'prepare',
                    'result': 'ready_resources'
                },
                {
                    'initial_state': 'destination, content, resources',
                    'transformation': 'deliver',
                    'result': 'delivery_status'
                }
            ]
        },
        
        {
            'source_text': '''async def fetch_api_data(url, params):
    response = await http_get(url, params)
    parsed = json.loads(response.text)
    return transform_schema(parsed)''',
            
            'title': 'API Data Fetcher',
            
            'extracted_chains': [
                {
                    'initial_state': 'url, params',
                    'transformation': 'http_get',
                    'result': 'response'
                },
                {
                    'initial_state': 'response.text',
                    'transformation': 'json.loads',
                    'result': 'parsed'
                },
                {
                    'initial_state': 'parsed',
                    'transformation': 'transform_schema',
                    'result': 'transformed_data'
                }
            ],
            
            'abstracted_chains': [
                {
                    'initial_state': 'endpoint, parameters',
                    'transformation': 'request',
                    'result': 'raw_response'
                },
                {
                    'initial_state': 'raw_response',
                    'transformation': 'parse',
                    'result': 'structured_data'
                },
                {
                    'initial_state': 'structured_data',
                    'transformation': 'transform',
                    'result': 'normalized_output'
                }
            ]
        }
    ]
    
    return examples


def prepare_for_hf(examples):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è HF"""
    data = {
        'source_text': [],
        'title': [],
        'extracted_chains': [],
        'abstracted_chains': []
    }
    
    for ex in examples:
        data['source_text'].append(ex['source_text'])
        data['title'].append(ex['title'])
        data['extracted_chains'].append(ex['extracted_chains'])
        data['abstracted_chains'].append(ex['abstracted_chains'])
    
    return data


def create_and_upload_dataset(hf_username, hf_token=None, private=True):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ Hugging Face
    
    Args:
        hf_username: –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ HF
        hf_token: —Ç–æ–∫–µ–Ω (–∏–ª–∏ None –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
        private: –ø—Ä–∏–≤–∞—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ –ø—É–±–ª–∏—á–Ω—ã–π
    """
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    examples = create_dataset_structure()
    data = prepare_for_hf(examples)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = Dataset.from_dict(data)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test (80/20)
    dataset_dict = dataset.train_test_split(test_size=0.2, seed=42)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç:")
    print(f"  Train: {len(dataset_dict['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"  Test: {len(dataset_dict['test'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞:")
    print(dataset_dict['train'].features)
    print(f"\n–ü–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä:")
    print(dataset_dict['train'][0])
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ HF
    repo_id = f"{hf_username}/epistack-optimization"
    
    if hf_token is None:
        hf_token = os.getenv('HF_TOKEN')
    
    if hf_token:
        print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hugging Face: {repo_id}")
        dataset_dict.push_to_hub(
            repo_id,
            private=private,
            token=hf_token
        )
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: https://huggingface.co/datasets/{repo_id}")
    else:
        print("\n‚ö†Ô∏è  HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω –ª–æ–∫–∞–ª—å–Ω–æ.")
        print("–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ HF:")
        print("  1. –ü–æ–ª—É—á–∏ —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens")
        print("  2. export HF_TOKEN='your_token'")
        print(f"  3. –ó–∞–ø—É—Å—Ç–∏ —Å–Ω–æ–≤–∞")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
        dataset_dict.save_to_disk('datasets/epistack_optimization_local')
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ: datasets/epistack_optimization_local")
    
    return dataset_dict


def load_dataset_example(hf_username, hf_token=None):
    """–ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    from datasets import load_dataset
    
    repo_id = f"{hf_username}/epistack-optimization"
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å HF
        ds = load_dataset(repo_id, token=hf_token)
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —Å HF: {repo_id}")
    except:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        ds = load_dataset('datasets/epistack_optimization_local')
        print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
    
    return ds


if __name__ == '__main__':
    # –ß–∏—Ç–∞–µ–º –∏–∑ .env
    from dotenv import load_dotenv
    load_dotenv()
    
    HF_USERNAME = os.getenv('HF_USERNAME', 'Nick-Sen')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
    dataset = create_and_upload_dataset(
        hf_username=HF_USERNAME,
        private=False  # –ü—É–±–ª–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    )
    
    print("\n" + "="*60)
    print("–ì–æ—Ç–æ–≤–æ! –ò—Å–ø–æ–ª—å–∑—É–π –¥–∞—Ç–∞—Å–µ—Ç:")
    print("="*60)
    print(f"""
from datasets import load_dataset

# –ó–∞–≥—Ä—É–∑–∫–∞
ds = load_dataset('{HF_USERNAME}/epistack-optimization', token='your_token')

# –í pandas
df = ds['train'].to_pandas()

# –î–ª—è DSPy
import dspy
trainset = [dspy.Example(**x).with_inputs('source_text') for x in ds['train']]
""")

