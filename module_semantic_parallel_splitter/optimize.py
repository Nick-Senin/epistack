"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è SemanticHalver
"""
import os
import json
import dspy
from .module import SemanticHalver
from .config import configure_module_llm

DSPY_LM_DOCS_URL = "https://dspy.ai"


def _lm_output_to_text(output):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ—Ç–≤–µ—Ç LLM –∫ —Å—Ç—Ä–æ–∫–µ.

    GEPA –æ–∂–∏–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∏ –≤—ã–∑—ã–≤–∞–µ—Ç .strip(). –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç dict/list.
    """
    if isinstance(output, dict):
        return (output.get("text") or output.get("completion") or "").strip()
    if isinstance(output, list):
        if not output:
            return ""
        first = output[0]
        if isinstance(first, dict):
            return (first.get("text") or first.get("completion") or "").strip()
        return str(first).strip()
    return str(output).strip()


class _TextOnlyLM:
    """–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è LLM, —á—Ç–æ–±—ã GEPA –≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞–ª —Å—Ç—Ä–æ–∫—É."""

    def __init__(self, lm):
        self._lm = lm
        self.model = getattr(lm, "model", "unknown")

    def __call__(self, prompt, **kwargs):
        return _lm_output_to_text(self._lm(prompt, **kwargs))


def create_reflection_lm(
    model=None,
    api_base=None,
    api_key=None,
    **kwargs
):
    """
    –°–æ–∑–¥–∞—ë—Ç LM –¥–ª—è reflection (–æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤).

    Returns:
        _TextOnlyLM: LLM-–æ–±—ë—Ä—Ç–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è —Å—Ç—Ä–æ–∫—É
    """
    # –°–º. –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é DSPy –ø–æ LM:
    # https://dspy.ai
    model = (model or os.getenv("REFLECTION_MODEL") or os.getenv("CEREBRAS_MODEL") or "cerebras/zai-glm-4.7").strip()
    api_base = (api_base or os.getenv("REFLECTION_API_BASE") or os.getenv("CEREBRAS_API_BASE") or "https://api.cerebras.ai/v1").strip()
    api_key = (api_key or os.getenv("REFLECTION_API_KEY") or os.getenv("CEREBRAS_API_KEY") or "").strip()

    if not api_key:
        raise ValueError(
            "–ù–µ –Ω–∞–π–¥–µ–Ω API –∫–ª—é—á –¥–ª—è reflection-LM. –£–∫–∞–∂–∏—Ç–µ `REFLECTION_API_KEY` –∏–ª–∏ `CEREBRAS_API_KEY`."
        )

    lm = dspy.LM(
        model=model,
        api_base=api_base,
        api_key=api_key,
        **kwargs
    )

    wrapped_lm = _TextOnlyLM(lm)
    print(f"üîß Reflection LM configured: {wrapped_lm.model}")
    return wrapped_lm


class SemanticHalverMetric:
    """
    –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ SemanticHalver.

    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —Å–º—ã—Å–ª–æ–≤–æ–≥–æ –±–ª–æ–∫–∞.

    –î–ª—è GEPA –º–µ—Ç—Ä–∏–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å dspy.Prediction —Å –ø–æ–ª—è–º–∏:
    - score: –æ—Ü–µ–Ω–∫–∞ –æ—Ç 0.0 –¥–æ 1.0
    - feedback: —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∏–¥–±–µ–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫–∏."""
        pass

    def __call__(self, example, pred, trace=None, pred_name=None, pred_trace=None):
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.

        Args:
            example: –ü—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–ª—è–º–∏:
                - text: –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (part1 + part2)
                - ground_truth_first_block: –æ–∂–∏–¥–∞–µ–º—ã–π –ø–µ—Ä–≤—ã–π –±–ª–æ–∫ (part1)
            pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥—É–ª—è —Å –ø–æ–ª—è–º–∏:
                - first_block: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–≤—ã–π –±–ª–æ–∫
                - split_index: –∏–Ω–¥–µ–∫—Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
            trace: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç—Ä–µ–π—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            pred_name: –ò–º—è –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ (–¥–ª—è GEPA)
            pred_trace: –¢—Ä–µ–π—Å –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ (–¥–ª—è GEPA)

        Returns:
            float: –û—Ü–µ–Ω–∫–∞ –æ—Ç 0.0 –¥–æ 1.0 (–∏–ª–∏ dspy.Prediction –¥–ª—è GEPA)
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º ground truth –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            ground_truth = example.ground_truth_first_block.strip()
            predicted = pred.first_block.strip()

            # –ï—Å–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ - –Ω–∏–∑–∫–∞—è –æ—Ü–µ–Ω–∫–∞
            if not predicted:
                return self._make_result(0.0, "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –±–ª–æ–∫ –ø—É—Å—Ç–æ–π")

            # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            if ground_truth == predicted:
                return self._make_result(1.0, "–ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            full_text = example.text
            if predicted not in full_text:
                # LLM –Ω–∞—Ä—É—à–∏–ª –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é "—Ç–æ—á–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∞"
                return self._make_result(
                    0.0,
                    f"LLM –Ω–∞—Ä—É—à–∏–ª –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é '—Ç–æ—á–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∞'. "
                    f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –±–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ. "
                    f"–û–∂–∏–¥–∞–ª–æ—Å—å: {ground_truth[:100]}... "
                    f"–ü–æ–ª—É—á–µ–Ω–æ: {predicted[:100]}..."
                )

            # –í—ã—á–∏—Å–ª—è–µ–º overlap –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑–∏—Ü–∏–π
            gt_len = len(ground_truth)
            pred_len = len(predicted)

            # –ï—Å–ª–∏ –¥–ª–∏–Ω—ã —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è - –Ω–∏–∑–∫–∞—è –æ—Ü–µ–Ω–∫–∞
            len_diff = abs(gt_len - pred_len) / max(gt_len, pred_len)
            if len_diff > 0.5:  # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ > 50%
                return self._make_result(
                    0.1,
                    f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–ª–∏–Ω–µ. "
                    f"–û–∂–∏–¥–∞–ª–æ—Å—å: {gt_len} —Å–∏–º–≤–æ–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ: {pred_len} —Å–∏–º–≤–æ–ª–æ–≤"
                )

            # –í—ã—á–∏—Å–ª—è–µ–º overlap –ø–æ —Å–∏–º–≤–æ–ª–∞–º
            # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –æ–±—â—É—é –ø–æ–¥—Å—Ç—Ä–æ–∫—É
            max_overlap = 0
            for i in range(min(gt_len, pred_len)):
                if ground_truth[:i] == predicted[:i]:
                    max_overlap = i
                else:
                    break

            overlap_ratio = max_overlap / max(gt_len, pred_len)

            # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–∞–∫–∂–µ —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–ª–∏–Ω–µ
            score = overlap_ratio * (1 - len_diff)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–¥–±–µ–∫
            feedback = (
                f"Overlap: {overlap_ratio:.2%}, —Ä–∞–∑–Ω–∏—Ü–∞ –¥–ª–∏–Ω—ã: {len_diff:.2%}. "
                f"–û–∂–∏–¥–∞–ª–æ—Å—å –±–ª–æ–∫ –¥–ª–∏–Ω–æ–π {gt_len}, –ø–æ–ª—É—á–µ–Ω–æ {pred_len}. "
                f"–°–æ–≤–ø–∞–ª–æ –ø–µ—Ä–≤—ã—Ö {max_overlap} —Å–∏–º–≤–æ–ª–æ–≤."
            )

            return self._make_result(max(0.0, min(1.0, score)), feedback)

        except Exception as e:
            return self._make_result(0.0, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫–∏: {e}")

    def _make_result(self, score, feedback):
        """
        –°–æ–∑–¥–∞—ë—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

        –î–ª—è GEPA –≤–æ–∑–≤—Ä–∞—â–∞–µ–º dspy.Prediction —Å feedback,
        –¥–ª—è –¥—Ä—É–≥–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –ø—Ä–æ—Å—Ç–æ float.
        """
        return dspy.Prediction(score=score, feedback=feedback)


def load_dataset(filepath=None, limit=15):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ JSON —Ñ–∞–π–ª–∞.

    Args:
        filepath: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º.
                   –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (None = –±–µ–∑ –ª–∏–º–∏—Ç–∞).

    Returns:
        list[dspy.Example]: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    if filepath is None:
        # –ü—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
        filepath = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "datasets",
            "splitting datasets",
            "split_chunks.json"
        )

    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {filepath}")

    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if limit is not None:
        data = data[:limit]

    examples = []
    for item in data:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ part1 –∏–ª–∏ part2
        if 'part1' not in item or 'part2' not in item:
            continue

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º part1 –∏ part2 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        full_text = item['part1'] + ' ' + item['part2']

        # –°–æ–∑–¥–∞—ë–º dspy.Example
        # text - –≤—Ö–æ–¥, ground_truth_first_block - –æ–∂–∏–¥–∞–µ–º—ã–π –≤—ã—Ö–æ–¥
        example = dspy.Example(
            text=full_text,
            ground_truth_first_block=item['part1']
        ).with_inputs('text')

        examples.append(example)

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    return examples


def optimize(
    dataset=None,
    max_metric_calls=50,
    optimizer_type='gepa',
    metric=None,
    valset_ratio=0.2,
    **kwargs
):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è SemanticHalver.

    Args:
        dataset: –î–∞—Ç–∞—Å–µ—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
                 –ï—Å–ª–∏ None, –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ —Ñ–∞–π–ª–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        max_metric_calls: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–∑–æ–≤–æ–≤ –º–µ—Ç—Ä–∏–∫–∏ (–Ω–µ –¥–ª—è GEPA)
        optimizer_type: –¢–∏–ø –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ ('gepa', 'mipro', 'bootstrap')
        metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SemanticHalverMetric)
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
                  –î–ª—è GEPA: auto="light"|"medium"|"heavy", num_threads, valset
        valset_ratio: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ valset –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω

    Returns:
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å
    """
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM
    configure_module_llm()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
    if dataset is None:
        dataset = load_dataset()

    if not dataset:
        raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

    # –°–æ–∑–¥–∞—ë–º –±–∞–∑–æ–≤—ã–π –º–æ–¥—É–ª—å
    student = SemanticHalver()

    # –°–æ–∑–¥–∞—ë–º –º–µ—Ç—Ä–∏–∫—É –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
    if metric is None:
        metric = SemanticHalverMetric()

    # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    if optimizer_type == 'gepa':
        # GEPA: Genealogical Effective Prompt Optimization
        reflection_lm = kwargs.pop('reflection_lm', None)
        if reflection_lm is None:
            reflection_lm = create_reflection_lm()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã GEPA
        auto = kwargs.pop('auto', 'light')
        num_threads = kwargs.pop('num_threads', 4)

        optimizer = dspy.GEPA(
            metric=metric,
            auto=auto,
            num_threads=num_threads,
            reflection_lm=reflection_lm,
            **kwargs
        )
    elif optimizer_type == 'mipro':
        # MIPRO: Multi-step Instruction Proposal Optimization
        num_trials = kwargs.pop('num_trials', 10)

        optimizer = dspy.MIPROv2(
            student=student,
            metric=metric,
            num_trials=num_trials,
            max_metric_calls=max_metric_calls,
            **kwargs
        )
    elif optimizer_type == 'bootstrap':
        # Bootstrap: Few-shot –ø—Ä–∏–º–µ—Ä—ã
        max_labeled_demos = kwargs.pop('max_labeled_demos', 8)
        max_rounds = kwargs.pop('max_rounds', 1)

        optimizer = dspy.BootstrapFewShot(
            metric=metric,
            max_labeled_demos=max_labeled_demos,
            max_rounds=max_rounds,
            **kwargs
        )
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: {optimizer_type}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ train/val –¥–ª—è GEPA
    valset = kwargs.pop('valset', None)
    trainset = dataset
    if optimizer_type == 'gepa' and valset is None:
        # –ü—Ä–æ—Å—Ç–æ–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–ª–∏—Ç: –ø–µ—Ä–≤—ã–µ N –¥–ª—è valset
        val_count = max(1, int(len(dataset) * valset_ratio))
        valset = dataset[:val_count]
        trainset = dataset[val_count:] or dataset

    # –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å {optimizer_type.upper()}...")
    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(dataset)}")
    if optimizer_type == 'gepa' and valset is not None:
        print(f"üß™ –†–∞–∑–º–µ—Ä valset: {len(valset)}; trainset: {len(trainset)}")

    if optimizer_type == 'gepa':
        # GEPA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥—Ä—É–≥–æ–π API: program positional, trainset, valset
        optimized_module = optimizer.compile(
            student,  # positional argument, not keyword
            trainset=trainset,
            valset=valset
        )
    else:
        # MIPRO –∏ BootstrapFewShot –∏—Å–ø–æ–ª—å–∑—É—é—Ç student, trainset
        print(f"üîÑ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã –º–µ—Ç—Ä–∏–∫–∏: {max_metric_calls}")
        optimized_module = optimizer.compile(
            student=student,
            trainset=dataset
        )

    print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return optimized_module


def save_optimized_module(module, filepath):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è.

    Args:
        module: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å
        filepath: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    # DSPy —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ prompt'—ã
    module.save(filepath)
    print(f"üíæ –ú–æ–¥—É–ª—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {filepath}")


def load_optimized_module(module_class, filepath):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è.

    Args:
        module_class: –ö–ª–∞—Å—Å –º–æ–¥—É–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, SemanticHalver)
        filepath: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–º—É –º–æ–¥—É–ª—é

    Returns:
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å
    """
    module = module_class()
    module.load(filepath)
    print(f"üìÇ –ú–æ–¥—É–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑: {filepath}")
    return module


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞/–º–æ–¥—É–ª—è.
    optimizer_type = os.getenv("OPTIMIZER_TYPE", "gepa").strip().lower()
    kwargs = {}
    if optimizer_type == "mipro":
        num_trials = os.getenv("NUM_TRIALS")
        if num_trials:
            kwargs["num_trials"] = int(num_trials)
    elif optimizer_type == "bootstrap":
        max_labeled_demos = os.getenv("MAX_LABELED_DEMOS")
        if max_labeled_demos:
            kwargs["max_labeled_demos"] = int(max_labeled_demos)
        max_rounds = os.getenv("MAX_ROUNDS")
        if max_rounds:
            kwargs["max_rounds"] = int(max_rounds)

    max_metric_calls = os.getenv("MAX_METRIC_CALLS")
    max_metric_calls = int(max_metric_calls) if max_metric_calls else 50

    optimized = optimize(
        optimizer_type=optimizer_type,
        max_metric_calls=max_metric_calls,
        **kwargs
    )
    default_save_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        "artifacts",
        "semantic_halver_optimized.json"
    )
    os.makedirs(os.path.dirname(default_save_path), exist_ok=True)
    save_optimized_module(optimized, default_save_path)
